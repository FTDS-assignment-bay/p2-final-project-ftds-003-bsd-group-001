{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "In the rapidly evolving landscape of retail and e-commerce, leveraging data to forecast profits is not just an advantage but a necessity. This project aims to harness the power of machine learning, specifically through the use of XGBoost, a leading gradient boosting framework, to predict profits from a comprehensive dataset provided by a superstore. The dataset encompasses various aspects of sales transactions, including product details, sales figures, profits, and customer information, meticulously cleaned and prepared for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Context\n",
    "\n",
    "Understanding and predicting profits enable businesses to make informed decisions, optimize operations, and enhance customer satisfaction. By accurately forecasting profit margins, the superstore can identify lucrative products, tailor marketing strategies, manage inventory more effectively, and ultimately, drive growth. This project serves as a strategic tool, guiding the superstore in navigating market dynamics, competitive pressures, and customer preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The primary objective of this project is to develop a robust predictive model that can forecast profit margins with high accuracy. By doing so, we aim to provide actionable insights that will assist the superstore in strategic planning, resource allocation, and performance optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMART Analysis\n",
    "\n",
    "To ensure the project's objectives are clearly defined and achievable, we employ the SMART criteria:\n",
    "\n",
    "- **Specific**: Develop a machine learning model using XGBoost to predict profits based on historical sales data. The model will consider variables such as product categories, sales channels (online or in-store), geographical regions, and customer segments.\n",
    "\n",
    "- **Measurable**: Achieve a model with a prediction accuracy of at least 90% on the validation set, utilizing metrics such as RMSE (Root Mean Square Error) or MAE (Mean Absolute Error) for quantifying performance.\n",
    "\n",
    "- **Achievable**: Given the comprehensive dataset and the proven capabilities of XGBoost in handling complex datasets and predictive tasks, developing a highly accurate predictive model is within reach.\n",
    "\n",
    "- **Relevant**: This project aligns with the superstore's strategic goals of maximizing profits, optimizing inventory management, and enhancing customer satisfaction through data-driven decisions.\n",
    "\n",
    "- **Time-bound**: The project will be completed in phases over a period of 3 months, starting from data preparation, model development, and testing, to final deployment and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance to the Business\n",
    "\n",
    "By successfully predicting profits, the superstore can proactively manage its operations, refine its marketing strategies, and improve customer engagement. This project not only aims to bolster the bottom line but also to foster a culture of innovation and data-centric decision-making within the organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import squarify\n",
    "import matplotlib.cm as cm\n",
    "import phik\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost as xg\n",
    "from xgboost import plot_importance\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from math import sqrt\n",
    "\n",
    "# Set pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our analysis involves loading the cleaned dataset from a CSV file. This dataset is a comprehensive collection of sales transactions from a superstore, including various attributes such as product details, sales figures, and profit margins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading cleaned data from csv file\n",
    "df=pd.read_csv('superstore_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon loading the data, it's essential to get a glimpse of its structure and the first few entries to understand the dataset's composition. Displaying the first five rows offers a preliminary view of the variables involved, such as order ID, order date, ship date, product category, sales, and profit, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output reveals the dataset's first five transactions, showcasing columns like `order_id`, `order_date`, `ship_mode`, `segment`, `category`, `sales`, `quantity`, `discount`, and `profit`. For instance, the first row indicates a sale made on *November 8, 2016*, involving furniture (a bookcase) with **a sale amount of $261.96**, resulting in a ***profit of $41.9136***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises 9994 entries across 21 columns, with data types ranging from integers and floats to objects (strings). Notably, there are **no null values** in any of the columns, indicating a well-maintained and clean dataset ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical summary of the numerical fields in the dataset provides insights into the distribution of sales, profits, quantities, and discounts. This summary includes measures of central tendency and dispersion, such as mean, median, standard deviation, minimum, and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying statistics for numerical data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical overview reveals key insights, such as the average sale amount being approximately $229.86, with a wide range of sales values indicated by the standard deviation of $623.25. Profits, on the other hand, have a mean of $28.66, but the standard deviation of $234.26 suggests significant variability. This summary underscores the dataset's diverse transactional dynamics, from small to large sales and profits, highlighting the potential for in-depth analysis on factors influencing sales and profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sales Across Different Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features\n",
    "df_bar = df[['region','sales']]\n",
    "df_bar = df_bar.groupby('region').mean().sort_values(by='sales', ascending=False)\n",
    "\n",
    "plt.figure(figsize=[15,8]) # Setting the figure size\n",
    "\n",
    "plt.suptitle(\"Average Sales Across Different Regions\", size=20, ha='center')  # Title is centered\n",
    "\n",
    "\n",
    "# Plotting the BarChart with Seaborn\n",
    "plt.subplot()\n",
    "sns.barplot(x=df_bar.index, y='sales', data=df_bar, palette='viridis')\n",
    "\n",
    "# Menambahkan label count ke dalam plot\n",
    "for index, value in enumerate(df_bar['sales']):\n",
    "    plt.text(index, value, str(int(value)), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The bar chart visualizes the average sales across different regions, revealing how sales performance varies geographically. The chart indicates that the Central region has the highest average sales, followed by the South, East, and West regions. This insight suggests regional preferences or operational strengths that could influence strategic decisions such as inventory distribution, marketing focus, and resource allocation to optimize sales performance in underperforming regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sales and Profit Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we are going to take only the subset of data for our purpose. (To keep things simple)\n",
    "df_line = df[['order_date','sales','profit']].sort_values('order_date') # Chronological Ordering\n",
    "df_line['order_date'] = pd.to_datetime(df_line['order_date']) # Converting into DateTime\n",
    "df_line = df_line.groupby('order_date').mean() # Groupby to get the average sales and profit on each day\n",
    "\n",
    "# Visualizing the Line Chart\n",
    "plt.figure(figsize=[15,8])\n",
    "plt.plot(df_line.index, 'sales', data=df_line, color='#F05454') # Avg sales over Time\n",
    "plt.plot(df_line.index, 'profit', data=df_line, color='#30475E') # Avg profit over Time\n",
    "plt.title(\"Average Sales and Profit over Time Period(2014-2018)\", size=20, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The line chart presents a temporal view of average sales and profits from 2014 to 2018, offering insights into the store's financial health and sales trends over time. Notably, while sales exhibit seasonal peaks and troughs, profit trends might indicate underlying operational efficiencies or cost management strategies. The visualization underscores the importance of aligning sales strategies with cost control to enhance profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment vs. Region Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_colors1 = plt.cm.viridis([0.2, 0.5, 0.8, 1.0])  # Membuat warna dari palet Viridis\n",
    "# Membuat kotak dasar untuk chart\n",
    "fig = plt.figure(figsize=(22,6))\n",
    "plt.suptitle('Segment Vs Region',weight='bold',fontsize=20)\n",
    "grid = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n",
    "\n",
    "# Membuat bar chart berisi count\n",
    "ax1 = fig.add_subplot(grid[0,:1])\n",
    "ax1.set_title('Segment Vs Region Distribution')\n",
    "# Menggunakan seaborn untuk menggambar chart\n",
    "sns.countplot(x='segment', data=df, ax=ax1, hue='region', palette=target_colors1)\n",
    "\n",
    "# Mencoba membuat pie chart untuk melihat komposisi data cab_type\n",
    "ax2 = fig.add_subplot(grid[0,1:])\n",
    "ax2.set_title('Distribution Percentage of Segment')\n",
    "# Memasukkan perhitungan values counts cab_type kedalam index\n",
    "label = list(df['segment'].value_counts().index)\n",
    "value = list(df['segment'].value_counts().values)\n",
    "\n",
    "# Membuat pie chart\n",
    "ax2.pie(value, labels=label, autopct='%1.1f%%', colors=target_colors1)\n",
    "# Menunjukkan chart yang sudah dibuat\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count plot and pie chart illustrate the distribution of customer segments across different regions and the overall composition of these segments. The visual analysis reveals a balanced distribution of segments across regions, with a particular emphasis on the consumer segment's predominance. These insights can guide targeted marketing strategies and product offerings to cater to the dominant segments in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Composition for Profit and Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,15])\n",
    "plt.subplot()\n",
    "\n",
    "# Taking a subset of data (To keep things simple)\n",
    "df_pie = df[['category','sales','profit']]\n",
    "# taking a groupby on category and then ship_mode...\n",
    "df_pie = df_pie.groupby(['category']).sum().reset_index()\n",
    "\n",
    "# Visualizing the Pie Chart (profit)\n",
    "plt.subplot(3,2,3)\n",
    "plt.pie(df_pie['profit'], labels=df_pie['category'], colors=['#F05454','#30475E','#222831','#DDDDDD'])\n",
    "plt.title(\"Composition of category for profit\")\n",
    "\n",
    "# Visualizing the Pie Chart (sales)\n",
    "plt.subplot(3,2,4)\n",
    "plt.pie(df_pie['sales'], labels=df_pie['category'], colors=['#F05454','#30475E','#222831','#DDDDDD'])\n",
    "plt.title(\"Composition of category for sales\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pie charts for profit and sales by category underscore the contribution of different product categories to the store's overall financial metrics. These charts reveal which categories are more profitable or generate higher sales, highlighting potential areas for inventory expansion or pricing strategy adjustments to maximize profitability and sales performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales Across Shipping Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a subset of data (To keep things simple)\n",
    "df_stackb = df[['ship_mode','sales','profit']]\n",
    "# taking a groupby on category and then ship_mode...\n",
    "df_stackb = df_stackb.groupby(['ship_mode']).sum().reset_index()\n",
    "\n",
    "# Visualizing the Stacked BarChart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Creating horizontal bar chart for sales\n",
    "ax.barh(y=df_stackb['ship_mode'], width=df_stackb['sales'], color=plt.cm.viridis(0.2))\n",
    "ax.set_title('Sales Across Ship Modes')\n",
    "ax.set_xlabel('Sales Amount')\n",
    "ax.set_ylabel('Ship Mode')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bar chart depicting sales across different shipping modes provides insights into customer preferences and operational logistics. Identifying the most popular shipping modes can help streamline logistics operations, improve customer satisfaction, and potentially reduce costs through optimized shipping strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales vs. Profit Across Customer Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scatter = df[['sales','profit','segment']]\n",
    "\n",
    "# Visualizing the ScatterPlot\n",
    "plt.figure(figsize=[15,8])\n",
    "# Profit in the Y axis, and Sales in the X. Hue will classify the dots according to Segment.\n",
    "# The size of the dots are according to the volumen of \"Sales\".\n",
    "sns.scatterplot(x=df_scatter['sales'], y=df_scatter['profit'], hue=df_scatter['segment'], palette='viridis', size=df_scatter[\"sales\"], sizes=(100,1000), legend='auto') \n",
    "plt.title(\"Sales vs Profit Across Different Customer Segments\", size=20, pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot explores the relationship between sales and profit across different customer segments, with the size of each point representing the volume of sales. This visualization highlights the variability in profitability across different sales volumes and segments, suggesting that while some segments may generate higher sales, the associated profits vary significantly. This insight is crucial for refining segmentation strategies, product offerings, and pricing models to enhance profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights from the phi_k Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the phik correlation matrix\n",
    "df_phik = df.phik_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phi_k correlation matrix provided offers a comprehensive overview of the relationships between various features of the superstore dataset. This analysis focuses on the significant correlations that could impact business strategies and decision-making processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the phik correlation heatmap\n",
    "plt.figure(figsize=(45,25))\n",
    "sns.heatmap(df_phik, annot=True, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Correlation between Order Attributes\n",
    "- **Order Date and Ship Date** have almost perfect correlation (\\(>0.99\\)), indicating that the time between placing an order and shipping is consistent across the dataset. This consistency can be leveraged for optimizing logistics and ensuring customer satisfaction through reliable delivery timeframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer and Segment Insights\n",
    "- **Customer ID and Customer Name** also show perfect correlation with several order-related attributes, highlighting the importance of understanding customer behaviors and preferences for targeted marketing and personalized customer experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geographic Influence\n",
    "- **City and State** correlations with postal code (\\(>0.99\\)) underline the geographical clustering of sales, which can be pivotal for regional marketing strategies and inventory distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Relationships\n",
    "- **Product ID, Category, and Sub-Category** show a strong interrelation, especially Product ID with Sales and Profit (\\(>0.87\\)), suggesting that specific products significantly drive the store's financial performance. This insight could guide inventory management, product development, and promotional efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales and Profit Dynamics\n",
    "- **Sales and Profit** demonstrate a high correlation (\\(0.89\\)), reinforcing the direct impact of sales performance on profitability. This relationship underscores the need for strategies that not only increase sales volume but also enhance profit margins, such as optimizing product pricing, promotions, and cost management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discounts Impact\n",
    "- **Discounts** show a notable correlation with several attributes, including a moderate correlation with profit (\\(0.21\\)). This suggests that while discounts can drive sales volume, they also have a tangible impact on profitability. Balancing discounts to attract customers while preserving profit margins is crucial for sustainable growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operational Insights\n",
    "- **Ship Mode** has a lower correlation with sales and profit, which may indicate that the mode of shipping has a less direct impact on financial outcomes. However, optimizing shipping strategies could still contribute to customer satisfaction and operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Collinearity Focused on Profit\n",
    "\n",
    "From the provided phi_k correlation matrix, we can observe the correlation values associated with the `profit` variable. The key is to identify any variables that show a strong correlation (\\(\\phi_k > 0.5\\)) with `profit`, indicating a potentially strong predictive relationship. Then, we'll assess if these variables are also highly correlated with each other, which would suggest collinearity.\n",
    "\n",
    "#### High Correlation with Profit\n",
    "- **Sales:** With a phi_k correlation of 0.893 with `profit`, it's clear that sales have a strong relationship with profit. This is expected as sales figures directly impact profit margins.\n",
    "\n",
    "- **Product ID, Product Name:** These variables show high correlations (phi_k of 0.819 and 0.822, respectively) with `profit`, indicating that specific products are consistently more profitable than others. This could reflect product-specific attributes like margins, popularity, or cost structures.\n",
    "\n",
    "- **Sub Category:** Shows a phi_k of approximately 0.32 with `profit`, suggesting a moderate relationship. This implies that certain sub-categories may be more profitable on average, likely due to differences in pricing strategies, cost, or consumer demand.\n",
    "\n",
    "#### Assessing Collinearity\n",
    "- **Sales and Product-Related Features:** Given the high correlation between `sales` and `profit`, and similarly high correlations of product-related features with `profit`, it's reasonable to infer a degree of collinearity among `sales`, `product_id`, and `product_name`. This suggests that the profitability of specific products is strongly tied to their sales performance.\n",
    "\n",
    "- **Product ID and Product Name:** As both `product_id` and `product_name` are essentially identifiers for the same attribute (the product), their high correlation with each other and with `profit` is expected and reflects a direct relationship rather than collinearity in the traditional sense affecting model performance.\n",
    "\n",
    "### Conclusion\n",
    "The analysis suggests a strong predictive relationship between `sales`, product identifiers, and `profit`. While `product_id` and `product_name` are naturally correlated with `profit`, their relationship reflects product-specific performance rather than an issue of collinearity affecting model stability. For linear models, it would be prudent to further investigate these relationships using VIF. However, for your XGBoost model focusing on `profit`, prioritizing feature importance and leveraging the model's inherent handling of collinearity may be more practical, focusing on how different features, including `sales` and product attributes, contribute to predicting `profit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing dates to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'order_date' and 'ship_date' to datetime format\n",
    "df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "df['ship_date'] = pd.to_datetime(df['ship_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in categorical columns for consistency\n",
    "categorical_columns = ['ship_mode', 'segment', 'country', 'region', 'category', 'sub_category']\n",
    "categorical_uniques = {column: df[column].unique() for column in categorical_columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numerical columns to identify potential outliers\n",
    "numerical_summary = df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Features Engineering\n",
    "df['order_year'] = df['order_date'].dt.year\n",
    "df['order_month'] = df['order_date'].dt.month\n",
    "df['order_day'] = df['order_date'].dt.day\n",
    "df['order_day_of_week'] = df['order_date'].dt.dayofweek\n",
    "df['is_weekend'] = df['order_day_of_week'].isin([5, 6]).astype(int)\n",
    "df['shipping_duration'] = (df['ship_date'] - df['order_date']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('row_id', axis=1, inplace=True)\n",
    "df.drop('order_id', axis=1, inplace=True)\n",
    "df.drop('order_date', axis=1, inplace=True)\n",
    "df.drop('ship_date', axis=1, inplace=True)\n",
    "df.drop('customer_id', axis=1, inplace=True)\n",
    "df.drop('customer_name', axis=1, inplace=True)\n",
    "df.drop('country', axis=1, inplace=True)\n",
    "df.drop('postal_code', axis=1, inplace=True)\n",
    "df.drop('product_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Unit Price Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unit price as sales divided by quantity\n",
    "df['unit_price'] = df['sales'] / df['quantity']\n",
    "\n",
    "# To illustrate the approach, let's take a look at the distribution of unit prices within a specific sub-category\n",
    "# We'll choose a sub-category with a diverse range of products. For demonstration, let's use \"Binders\"\n",
    "binders_unit_prices = df[df['sub_category'] == 'Binders']['unit_price']\n",
    "\n",
    "# Display basic statistics to understand the distribution of unit prices within \"Binders\"\n",
    "binders_unit_prices.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering to reduce cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def optimal_clusters(data, max_k=10):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Elbow Method and Silhouette Score.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, max_k + 1)\n",
    "    cluster_centers = {}\n",
    "\n",
    "    for K in K_range:\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42).fit(data)\n",
    "        scores.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(data, kmeans.labels_))\n",
    "\n",
    "    # Silhouette Method: Find the number of clusters with the highest silhouette score\n",
    "    best_silhouette_index = np.argmax(silhouette_scores)\n",
    "    best_k = K_range[best_silhouette_index]\n",
    "    best_kmeans = KMeans(n_clusters=best_k, random_state=42).fit(data)\n",
    "    \n",
    "    # Save the cluster centers for the best K\n",
    "    cluster_centers['centers'] = best_kmeans.cluster_centers_\n",
    "    cluster_centers['n_clusters'] = best_k\n",
    "\n",
    "    return best_kmeans.labels_, cluster_centers\n",
    "\n",
    "# Initialize a dictionary to store cluster centers for each sub-category\n",
    "sub_category_cluster_centers = {}\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "clustered_data = pd.DataFrame()\n",
    "for sub_category in df['sub_category'].unique():\n",
    "    sub_category_data = df[df['sub_category'] == sub_category].copy()\n",
    "    prices = sub_category_data[['unit_price']]\n",
    "\n",
    "    if len(prices) > 1:  # Ensure there's enough data for clustering\n",
    "        labels, centers = optimal_clusters(prices)\n",
    "        sub_category_data['price_cluster'] = labels\n",
    "        sub_category_cluster_centers[sub_category] = centers\n",
    "    else:\n",
    "        sub_category_data['price_cluster'] = 0  # Assign to a single cluster if not enough data\n",
    "\n",
    "    clustered_data = pd.concat([clustered_data, sub_category_data])\n",
    "\n",
    "# 'clustered_data' now includes the 'price_cluster' column, and 'sub_category_cluster_centers' stores the cluster centers for each sub-category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust cluster labels to be distinct across different sub-categories\n",
    "clustered_data['distinct_cluster_label'] = clustered_data['sub_category'] + '_' + clustered_data['price_cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing before and after clustering\n",
    "clustered_data.product_name.nunique()\n",
    "clustered_data.distinct_cluster_label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data.drop('product_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phik correlation\n",
    "phik_matrix = clustered_data.phik_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(phik_matrix, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the correlation with profit\n",
    "correlation_with_profit_phik = phik_matrix['profit'].sort_values(ascending=False)\n",
    "print(correlation_with_profit_phik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features\n",
    "features = ['sales', 'unit_price', 'discount', 'quantity', 'distinct_cluster_label', 'sub_category']\n",
    "\n",
    "# Target variable\n",
    "target = 'profit'\n",
    "\n",
    "# Splitting data into 75% train and 25% combined validation and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(clustered_data[features], clustered_data[target], test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col = X_train.select_dtypes(include = np.number).columns.tolist()  # select all the number columns and append them to n_col\n",
    "c_col = X_train.select_dtypes(include = ['object']).columns.tolist() # select all the object columns and append them to c_col\n",
    "print('Numerical Columns:', n_col)\n",
    "print('Categorical Columns:', c_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = X_train[n_col] #add the values of the columns to the test and train columns\n",
    "X_test_n = X_test[n_col]\n",
    "\n",
    "X_train_c = X_train[c_col]\n",
    "X_test_c = X_test[c_col]\n",
    "\n",
    "X_train_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #fit the Standard scaler and transform(scale) the numerical dataset\n",
    "scaler.fit(X_train_n) \n",
    "X_train_n_scaled = scaler.transform(X_train_n)\n",
    "X_test_n_scaled = scaler.transform(X_test_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard scaler is used to scale the dataset because the variables have various numerical measurements and that it is useful for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder() #encode the categorical column using onehotencoder\n",
    "\n",
    "X_train_c_encoded = ohe.fit_transform(X_train_c).toarray()\n",
    "X_test_c_encoded = ohe.transform(X_test_c).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is used because the categorical features are nominal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.concatenate([X_train_n_scaled,X_train_c_encoded], axis = 1) #combine the scaled and encoded columns\n",
    "X_test_final = np.concatenate([X_test_n_scaled,X_test_c_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xg.XGBRegressor() #Use XGB Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG Boost is one of the most popular and widely used regressors, it has the main advantage of high accuracy which makes the forecast more accurate. Other advantages is that, the algorithm uses L1 and  L2 regularization to reduce the model complexity which reduces overfitting. Thus making the regressor a primary choice for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(X_train_final, y_train) #fit the xgb regressor to the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = xgb.get_booster().get_score() #list the importance Score of each variable, the higher the score the more important the variable \n",
    "keys = list(feature_important.keys()) #list the keys and values to act as the labels for the graph\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "data.nlargest(20, columns=\"score\").plot(kind='barh', figsize = (20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    'feature': X_train.columns.tolist()\n",
    "}\n",
    "pd.DataFrame(params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the feature importance graph, the most important features of the graph scored accordingly via the F score is f0,f1 and f2. The reason why the feature names are like that because it is converted to a numpy array. However from the code above, it can be seen that f0 = sales etc. Thus sales, unit_price and discount has the most influence. There are also many variables listed because it is one hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = xgb.predict(X_train_final) #predict the model to the data\n",
    "y_pred_test = xgb.predict(X_test_final)\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model succesfully predict the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R2 of Train Set : ', r2_score(y_train, y_pred_train)) #tell the R^2 and RMSE score\n",
    "print('R2 of Test Set  : ', r2_score(y_test, y_pred_test))\n",
    "print('RMSE Train-set:', sqrt(mean_squared_error(y_train,y_pred_train)))\n",
    "print('RMSE Test-set:', sqrt(mean_squared_error(y_test,y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 from the test set is 0.91, with an acceptable root mean squared error of 52. This indicates that the features can explain 91% of movement of the target variable. This mean that this model have high significance of fit and therefore is accurate to predict the price from the given variables. However it is a bit overfit as the R^2 of train and test differs for 8%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming y_test and y_pred_test are your actual and predicted values respectively\n",
    "\n",
    "# Create a DataFrame for actual values\n",
    "actual_data = pd.DataFrame({\n",
    "    'Values': y_test,\n",
    "    'Index': range(len(y_test)),\n",
    "    'Type': 'Actual'\n",
    "})\n",
    "\n",
    "# Create a DataFrame for predicted values\n",
    "predicted_data = pd.DataFrame({\n",
    "    'Values': y_pred_test,\n",
    "    'Index': range(len(y_pred_test)),\n",
    "    'Type': 'Predicted'\n",
    "})\n",
    "\n",
    "# Combine both DataFrames\n",
    "combined_data = pd.concat([actual_data, predicted_data])\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Plot predicted values first\n",
    "sns.scatterplot(data=combined_data[combined_data['Type'] == 'Predicted'], x='Index', y='Values', color='lightblue', label='Predicted', alpha=0.6)\n",
    "# Then plot actual values on top so they are more prominent\n",
    "sns.scatterplot(data=combined_data[combined_data['Type'] == 'Actual'], x='Index', y='Values', color='darkorange', label='Actual', alpha=0.8)\n",
    "\n",
    "plt.title('Overlay of Actual and Predicted Values')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming `kmeans_model` is your trained KMeans model for clustering\n",
    "# `preprocessor` is your preprocessing pipeline (e.g., for scaling, encoding)\n",
    "# `model` is your final predictive model (e.g., XGBoost)\n",
    "\n",
    "joblib.dump(kmeans_model, 'kmeans_model.pkl')\n",
    "joblib.dump(, 'preprocessor.pkl')\n",
    "joblib.dump(xgb, 'xgboost_model.pkl')\n",
    "joblib.dump(sub_category_cluster_centers, 'sub_category_cluster_centers.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf = { #make new dataframe for inference\n",
    "'ship_mode': 'Second Class', \n",
    "'segment': 'Consumer', \n",
    "'city': 'San Diego', \n",
    "'state':'California', \n",
    "'region':'West', \n",
    "'category':'Furniture',\n",
    "'sub_category': 'Tables', \n",
    "'product_name':'Bretford CR4500 Series Slim Rectangular Table', \n",
    "'sales': 276, \n",
    "'quantity': 5,\n",
    "'discount': 0.1,\n",
    "'order_year':2016, \n",
    "'order_month':11, \n",
    "'order_day':8, \n",
    "'order_day_of_week':2,\n",
    "'is_weekend': 1, \n",
    "'shipping_duration':7, \n",
    "'unit_price':23,\n",
    "'price_cluster':1,\n",
    "'distinct_cluster_label': 'Fasteners_0'\n",
    "}\n",
    "\n",
    "df_inf = pd.DataFrame([df_inf])\n",
    "df_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf_n = df_inf[n_col]\n",
    "df_inf_c = df_inf[c_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf_n_scaled = scaler.transform(df_inf_n)\n",
    "df_inf_c_encoded = ohe.transform(df_inf_c).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf_final = np.concatenate([df_inf_n_scaled,df_inf_c_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inf = xgb.predict(df_inf_final) #from the data, the profit will be 35.6\n",
    "print('profit:', round(y_pred_inf[0],2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
